# -*- coding: utf-8 -*-
"""Q-DFTNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-kIRqv7k8QrF4BSIwRgn-GqE09N4R3BF

### Q-DFTNet Framework for Predicting Molecular Dipole Moments
"""

!pip install torch torchvision torchaudio
!pip install torch-scatter torch-sparse torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

# 1. Install PyTorch Geometric dependencies if not already installed
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install torch-geometric

import torch
import torch.nn.functional as F
from torch.nn import Linear
from torch_geometric.datasets import QM9
from torch_geometric.loader import DataLoader
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import numpy as np
import os
import pandas as pd
import json

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
batch_size = 32
epochs = 100
target_index = 0  # Œº (dipole moment)

os.makedirs("results", exist_ok=True)

path = './qm9'
dataset = QM9(path)
dataset = dataset.shuffle()

# Normalize target
mean = dataset.data.y[:, target_index].mean().item()
std = dataset.data.y[:, target_index].std().item()

def normalize(t): return (t - mean) / std
def denormalize(t): return t * std + mean

# Split
train_dataset = dataset[:10000]
val_dataset = dataset[10000:11000]
test_dataset = dataset[11000:12000]

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

from torch_geometric.nn import GCNConv, GINConv, GATConv, GraphConv, SAGEConv, EdgeConv, global_mean_pool
from torch.nn import Linear, Sequential, ReLU
import torch.nn.functional as F

def get_model(model_name, in_dim):
    import torch.nn.functional as F
    from torch.nn import Linear, Sequential, ReLU
    from torch_geometric.nn import GCNConv, GINConv, GATConv, GraphConv, SAGEConv, EdgeConv, global_mean_pool

    class BaseModel(torch.nn.Module):
        def forward_common(self, x, edge_index, batch):
            raise NotImplementedError

        def forward(self, x, edge_index, batch, return_latent=False):
            x = self.forward_common(x, edge_index, batch)
            x = F.relu(self.lin1(x))
            if return_latent:
                return x
            return self.lin2(x)

    if model_name == "GCN":
        class Net(BaseModel):
            def __init__(self):
                super().__init__()
                self.conv1 = GCNConv(in_dim, 64)
                self.conv2 = GCNConv(64, 64)
                self.lin1 = Linear(64, 32)
                self.lin2 = Linear(32, 1)

            def forward_common(self, x, edge_index, batch):
                x = F.relu(self.conv1(x, edge_index))
                x = F.relu(self.conv2(x, edge_index))
                return global_mean_pool(x, batch)
        return Net()

    if model_name == "GIN":
        class Net(BaseModel):
            def __init__(self):
                super().__init__()
                self.conv1 = GINConv(Sequential(Linear(in_dim, 64), ReLU(), Linear(64, 64)))
                self.conv2 = GINConv(Sequential(Linear(64, 64), ReLU(), Linear(64, 64)))
                self.lin1 = Linear(64, 32)
                self.lin2 = Linear(32, 1)

            def forward_common(self, x, edge_index, batch):
                x = F.relu(self.conv1(x, edge_index))
                x = F.relu(self.conv2(x, edge_index))
                return global_mean_pool(x, batch)
        return Net()

    if model_name == "GATNet" or model_name == "GATConv":
        class Net(BaseModel):
            def __init__(self):
                super().__init__()
                self.conv1 = GATConv(in_dim, 64, heads=1)
                self.conv2 = GATConv(64, 64, heads=1)
                self.lin1 = Linear(64, 32)
                self.lin2 = Linear(32, 1)

            def forward_common(self, x, edge_index, batch):
                x = F.elu(self.conv1(x, edge_index))
                x = F.elu(self.conv2(x, edge_index))
                return global_mean_pool(x, batch)
        return Net()

    if model_name == "SAGEConv":
        class Net(BaseModel):
            def __init__(self):
                super().__init__()
                self.conv1 = SAGEConv(in_dim, 64)
                self.conv2 = SAGEConv(64, 64)
                self.lin1 = Linear(64, 32)
                self.lin2 = Linear(32, 1)

            def forward_common(self, x, edge_index, batch):
                x = F.relu(self.conv1(x, edge_index))
                x = F.relu(self.conv2(x, edge_index))
                return global_mean_pool(x, batch)
        return Net()

    if model_name == "GraphConv":
        class Net(BaseModel):
            def __init__(self):
                super().__init__()
                self.conv1 = GraphConv(in_dim, 64)
                self.conv2 = GraphConv(64, 64)
                self.lin1 = Linear(64, 32)
                self.lin2 = Linear(32, 1)

            def forward_common(self, x, edge_index, batch):
                x = F.relu(self.conv1(x, edge_index))
                x = F.relu(self.conv2(x, edge_index))
                return global_mean_pool(x, batch)
        return Net()

    if model_name == "GIN+EdgeConv":
        class Net(BaseModel):
            def __init__(self):
                super().__init__()
                self.conv1 = GINConv(Sequential(Linear(in_dim, 64), ReLU(), Linear(64, 64)))
                self.conv2 = EdgeConv(Sequential(Linear(2 * 64, 64), ReLU(), Linear(64, 64)))
                self.lin1 = Linear(64, 32)
                self.lin2 = Linear(32, 1)

            def forward_common(self, x, edge_index, batch):
                x = F.relu(self.conv1(x, edge_index))
                x = F.relu(self.conv2(x, edge_index))
                return global_mean_pool(x, batch)
        return Net()

    raise ValueError(f"Unknown model: {model_name}")


def train_one_epoch(model, loader, optimizer, loss_fn):
    model.train()
    total_loss = 0
    for batch in loader:
        batch = batch.to(device)
        optimizer.zero_grad()
        y = normalize(batch.y[:, target_index]).unsqueeze(1)
        pred = model(batch.x, batch.edge_index, batch.batch)
        loss = loss_fn(pred, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * batch.num_graphs
    return total_loss / len(loader.dataset)

def evaluate(model, loader):
    model.eval()
    total_loss = 0
    y_true, y_pred = [], []
    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            y = normalize(batch.y[:, target_index]).unsqueeze(1)
            pred = model(batch.x, batch.edge_index, batch.batch)
            total_loss += F.mse_loss(pred, y, reduction='sum').item()
            y_true.append(batch.y[:, target_index].unsqueeze(1).cpu())  # unnormalized
            y_pred.append((pred.cpu() * std + mean))  # de-normalized

    y_true = torch.cat(y_true).numpy()
    y_pred = torch.cat(y_pred).numpy()

    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    return mse, mae, r2, y_true, y_pred

models_to_run = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
loss_fn = torch.nn.MSELoss()

for model_name in models_to_run:
    print(f"\n Training {model_name}")
    model = get_model(model_name, dataset.num_node_features).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)
    train_losses, val_losses, test_losses = [], [], []

    for epoch in range(1, epochs + 1):
        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn)
        val_mse, val_mae, val_r2, _, _ = evaluate(model, val_loader)
        test_mse, _, _, _, _ = evaluate(model, test_loader)

        train_losses.append(train_loss)
        val_losses.append(val_mse)
        test_losses.append(test_mse)

        print(f"Epoch {epoch:03d} | Train: {train_loss:.4f} | Val MSE: {val_mse:.4f} | Test MSE: {test_mse:.4f}")


    # Final test evaluation
    test_mse, test_mae, test_r2, y_true, y_pred = evaluate(model, test_loader)
    print(f" {model_name} | Test MSE: {test_mse:.4f} | MAE: {test_mae:.4f} | R¬≤: {test_r2:.4f}")

    # Save metrics
    results = {
        "Model": model_name,
        "Test_MSE": round(test_mse, 6),
        "Test_MAE": round(test_mae, 6),
        "Test_R2": round(test_r2, 6)
    }

    # Append to CSV/JSON
    csv_path = "results/model_results.csv"
    if os.path.exists(csv_path):
        df = pd.read_csv(csv_path)
        df = df[df["Model"] != model_name]
        df = pd.concat([df, pd.DataFrame([results])], ignore_index=True)
    else:
        df = pd.DataFrame([results])
    df.to_csv(csv_path, index=False)

    json_path = "results/model_results.json"
    if os.path.exists(json_path):
        with open(json_path, "r") as f:
            data = json.load(f)
        data = [d for d in data if d["Model"] != model_name]
        data.append(results)
    else:
        data = [results]
    with open(json_path, "w") as f:
        json.dump(data, f, indent=4)

    # Save predictions and loss curves
    np.save(f"results/{model_name}_y_true.npy", y_true)
    np.save(f"results/{model_name}_y_pred.npy", y_pred)
    np.save(f"results/{model_name}_mean_std.npy", np.array([mean, std]))
    np.save(f"results/{model_name}_train_loss.npy", np.array(train_losses))
    np.save(f"results/{model_name}_val_loss.npy", np.array(val_losses))
    np.save(f"results/{model_name}_test_loss.npy", np.array(test_losses))


    # Save latent graph embeddings
    model.eval()
    h_latent = []
    with torch.no_grad():
        for batch in test_loader:
            batch = batch.to(device)
            try:
                h = model(batch.x, batch.edge_index, batch.batch, return_latent=True)
            except TypeError:
                print(f" {model_name} does not support return_latent=True ‚Äî skipping.")
                continue
            h_latent.append(h.cpu())

    latent_array = torch.cat(h_latent).numpy()
    np.save(f"results/{model_name}_latent.npy", latent_array)
    print(f"Latent features saved to: results/{model_name}_latent.npy")

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from matplotlib_inline.backend_inline import set_matplotlib_formats
set_matplotlib_formats('svg')
sns.set_style("whitegrid")

# Font settings
plt.rcParams.update({
    'font.size': 14,
    'axes.titlesize': 16,
    'axes.labelsize': 15,
    'xtick.labelsize': 13,
    'ytick.labelsize': 13,
    'legend.fontsize': 12,
    'figure.titlesize': 17
})

models = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
plt.figure(figsize=(15, 9))

for model in models:
    try:
        train_loss = np.load(f"results/{model}_train_loss.npy")
        val_loss = np.load(f"results/{model}_val_loss.npy")
        test_loss = np.load(f"results/{model}_test_loss.npy")

        plt.plot(train_loss, label=f"{model} Train", linestyle='-')
        plt.plot(val_loss, label=f"{model} Val", linestyle='--')
        plt.plot(test_loss, label=f"{model} Test", linestyle=':')

    except FileNotFoundError:
        print(f" Missing logs for {model}")
        continue

plt.xlabel("Epoch")
plt.ylabel("Loss (MSE)")
plt.title("Training, Validation & Test Loss Curves for All GNN Models")

# Adjust legend
plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)
plt.tight_layout()
plt.grid(True)

# Save high-resolution image
plt.savefig("results/loss_curves_all.png", dpi=300, bbox_inches='tight')
plt.show()

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error

model_names = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]

results = []

for model in model_names:
    print(f"üîç Extracting MAE for {model}")
    try:
        y_true = np.load(f"results/{model}_y_true.npy").squeeze()
        y_pred = np.load(f"results/{model}_y_pred.npy").squeeze()
        mae = mean_absolute_error(y_true, y_pred)
        results.append({"Model": model, "MAE": round(mae, 6)})
    except Exception as e:
        print(f" Failed for {model}: {e}")

# Save to CSV
mae_df = pd.DataFrame(results)
mae_df.to_csv("results/model_mae_summary.csv", index=False)
print("MAE summary saved to results/model_mae_summary.csv")

print(mae_df)

import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score

def report_model_performance(results_dir="results", models=None):
    if models is None:
        models = [
            "GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"
        ]

    # Clean up previous figures
    plt.close('all')

    # Prepare plot
    cols = 4
    rows = (len(models) + cols - 1) // cols
    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 4 * rows))
    axes = axes.flatten()

    for i, model in enumerate(models):
        try:
            y_true = np.load(f"{results_dir}/{model}_y_true.npy")
            y_pred = np.load(f"{results_dir}/{model}_y_pred.npy")
            mean, std = np.load(f"{results_dir}/{model}_mean_std.npy")

            # De-normalize
            y_true = y_true * std + mean
            y_pred = y_pred * std + mean

            # Metrics
            mse = mean_squared_error(y_true, y_pred)
            r2 = r2_score(y_true, y_pred)
            print(f"{model} ‚Äî MSE: {mse:.4f} | R¬≤: {r2:.4f}")

            # Plot
            ax = axes[i]
            ax.scatter(y_true, y_pred, alpha=0.6)
            ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', lw=2)
            ax.set_title(f"{model} (R¬≤={r2:.2f})")
            ax.set_xlabel("Ground Truth")
            ax.set_ylabel("Prediction")

        except Exception as e:
            print(f"‚ùå Error for {model}: {e}")
            axes[i].set_visible(False)

    for j in range(len(models), len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.suptitle("Prediction vs. Ground Truth", fontsize=16, y=1.03)
    plt.savefig("results/predict_vs_groundtruth_realdata.png", dpi=300, bbox_inches='tight')
    plt.show()

# Call the function
report_model_performance()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

models = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
mses, r2s, maes = [], [], []

for model in models:
    try:
        y_true = np.load(f"results/{model}_y_true.npy")
        y_pred = np.load(f"results/{model}_y_pred.npy")
        mse = mean_squared_error(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        mses.append(mse)
        maes.append(mae)
        r2s.append(r2)
    except Exception as e:
        print(f" Could not load results for {model}: {e}")
        mses.append(np.nan)
        maes.append(np.nan)
        r2s.append(np.nan)

# Plot
x = np.arange(len(models))
width = 0.25

fig, ax = plt.subplots(figsize=(14, 6))
bars1 = ax.bar(x - width, mses, width, label="MSE")
bars2 = ax.bar(x, maes, width, label="MAE")
bars3 = ax.bar(x + width, r2s, width, label="R¬≤")

ax.set_xlabel("Graph Neural Network Model")
ax.set_ylabel("Metric Value")
ax.set_title("Test Performance on QM9 Dipole Moment Prediction")
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend()

# Annotate bars
for bars in [bars1, bars2, bars3]:
    for bar in bars:
        yval = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f'{yval:.2f}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.grid(True, axis='y', linestyle='--', alpha=0.4)
plt.savefig("results/test_metrics_barplot.png", dpi=400, bbox_inches='tight')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

models = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
tsne_embeddings = {}

for model in models:
    try:
        latent = np.load(f"results/{model}_latent.npy")
        y_true = np.load(f"results/{model}_y_true.npy").squeeze()

        if latent.shape[1] < 2:
            print(f" {model} latent vector too small for t-SNE")
            continue

        tsne = TSNE(n_components=2, perplexity=30, learning_rate=150, random_state=42)
        embedding = tsne.fit_transform(latent)
        tsne_embeddings[model] = (embedding, y_true)

    except Exception as e:
        print(f" Skipping {model}: {e}")

# === Plotting ===
fig, axes = plt.subplots(2, 4, figsize=(20, 10))
axes = axes.flatten()

for i, (model, (emb, y)) in enumerate(tsne_embeddings.items()):
    ax = axes[i]
    sc = ax.scatter(emb[:, 0], emb[:, 1], c=y, cmap='plasma', s=30, alpha=0.8)
    ax.set_title(f"{model} Latent Space", fontsize=14)
    ax.set_xlabel("TSNE-1")
    ax.set_ylabel("TSNE-2")

# Hide unused plots
for j in range(len(tsne_embeddings), len(axes)):
    fig.delaxes(axes[j])

# Colorbar
cbar = fig.colorbar(sc, ax=axes.tolist(), shrink=0.7)
cbar.set_label("True Dipole Moment (Œº)", fontsize=12)

plt.tight_layout()
plt.savefig("results/tsne_latent_vis.png", dpi=400)
plt.show()

import numpy as np
import pandas as pd
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# === Models you have ===
model_names = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]

results = []

for model in model_names:
    print(f"Processing {model}")

    # Load latent features and true dipole values
    latent = np.load(f"results/{model}_latent.npy")
    dipoles = np.load(f"results/{model}_y_true.npy").squeeze()

    # t-SNE projection
    tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=42)
    tsne_emb = tsne.fit_transform(latent)

    # KMeans clustering
    kmeans = KMeans(n_clusters=5, random_state=42)
    cluster_labels = kmeans.fit_predict(tsne_emb)

    # Clustering metrics
    silhouette = silhouette_score(tsne_emb, cluster_labels)
    db_index = davies_bouldin_score(tsne_emb, cluster_labels)
    ch_score = calinski_harabasz_score(tsne_emb, cluster_labels)

    # Store result
    results.append({
        "Model": model,
        "Silhouette Score": round(silhouette, 4),
        "Davies-Bouldin Index": round(db_index, 4),
        "Calinski-Harabasz Score": round(ch_score, 2)
    })

# Save to CSV
df = pd.DataFrame(results)
df.to_csv("results/clustering_scores.csv", index=False)
print("Clustering scores saved to results/clustering_scores.csv")

# Preview
print(df)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.ticker import FixedLocator

# Load scores
df = pd.read_csv("results/clustering_scores.csv")
sns.set(style="whitegrid")

# Create subplots
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Plot 1: Silhouette Score
sns.barplot(data=df, x="Model", y="Silhouette Score", hue="Model", palette="viridis", ax=axes[0], legend=False)
axes[0].set_title("Silhouette Score")
axes[0].set_ylim(0, 1)

# Plot 2: Davies-Bouldin Index (lower is better)
sns.barplot(data=df, x="Model", y="Davies-Bouldin Index", hue="Model", palette="magma", ax=axes[1], legend=False)
axes[1].set_title("Davies-Bouldin Index")
axes[1].invert_yaxis()

# Plot 3: Calinski-Harabasz Score (higher is better)
sns.barplot(data=df, x="Model", y="Calinski-Harabasz Score", hue="Model", palette="cubehelix", ax=axes[2], legend=False)
axes[2].set_title("Calinski-Harabasz Score")

# Format ticks
for ax in axes:
    ax.xaxis.set_major_locator(FixedLocator(range(len(df["Model"]))))
    ax.set_xticklabels(df["Model"], rotation=30)
    ax.grid(True, linestyle="--", alpha=0.6)

plt.tight_layout()
plt.savefig("results/cluster_score_bars.png", dpi=300)
plt.show()

import os
import urllib.request
import tarfile

# Define the URL and target paths
url = "http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/gdb9.tar.gz"
raw_folder = "qm9/raw"
os.makedirs(raw_folder, exist_ok=True)
tar_path = os.path.join(raw_folder, "gdb9.tar.gz")

# Download the tar.gz file
if not os.path.exists(tar_path):
    print(" Downloading gdb9.tar.gz...")
    urllib.request.urlretrieve(url, tar_path)
    print(f"Downloaded to: {tar_path}")
else:
    print("gdb9.tar.gz already exists.")

# Extract the tar.gz file
print(" Extracting gdb9.tar.gz...")
with tarfile.open(tar_path, "r:gz") as tar:
    tar.extractall(path=raw_folder)
print(f"Extracted to: {raw_folder}")

import pandas as pd

df = pd.read_csv("qm9/raw/gdb9.sdf.csv")
print("Columns in gdb9.sdf.csv:")
print(df.columns.tolist())

# Install RDKit in Colab
!pip install -q rdkit-pypi
!pip install umap-learn
!pip install cairosvg
!pip install moviepy

from rdkit import Chem # !!!!! run this cell two times
import pandas as pd
import os

sdf_path = "qm9/raw/gdb9.sdf"
output_csv = "results/smiles_test.csv"
os.makedirs("results", exist_ok=True)

# Correct version with fallback for invalid molecules
smiles = []
with Chem.SDMolSupplier(sdf_path, sanitize=False) as supplier:
    for i, mol in enumerate(supplier):
        if 9000 <= i < 10000:
            try:
                Chem.SanitizeMol(mol)
                smiles.append(Chem.MolToSmiles(mol))
            except:
                smiles.append(None)  # Maintain alignment

# Drop invalid SMILES (optional)
valid_smiles = [s for s in smiles if s is not None]
pd.DataFrame({"SMILES": valid_smiles}).to_csv(output_csv, index=False)
print(f"Saved {len(valid_smiles)} valid SMILES to {output_csv}")

import numpy as np
import pandas as pd
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans

model_names = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
smiles_list = pd.read_csv("results/smiles_test.csv")["SMILES"].tolist()

for model_name in model_names:
    print(f"Processing {model_name}")
    try:
        dipoles = np.load(f"results/{model_name}_y_true.npy").squeeze()
        latent = np.load(f"results/{model_name}_latent.npy")

        # t-SNE
        tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=42)
        tsne_emb = tsne.fit_transform(latent)

        # KMeans
        kmeans = KMeans(n_clusters=5, random_state=42)
        cluster_labels = kmeans.fit_predict(tsne_emb)

        # DataFrame
        df = pd.DataFrame({
            "SMILES": smiles_list,
            "DipoleMoment_Œº": dipoles,
            "ClusterID": cluster_labels,
            "TSNE-1": tsne_emb[:, 0],
            "TSNE-2": tsne_emb[:, 1]
        })

        df.to_csv(f"results/{model_name}_tsne_clustered.csv", index=False)
        print(f"Saved: results/{model_name}_tsne_clustered.csv")

    except Exception as e:
        print(f" Failed on {model_name}: {e}")
# Show preview
print(df.head())

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Set plotting style
sns.set(style="whitegrid")

models = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
fig, axes = plt.subplots(4, 2, figsize=(16, 20))
axes = axes.flatten()

for idx, model in enumerate(models):
    csv_path = f"results/{model}_tsne_clustered.csv"
    if not os.path.exists(csv_path):
        print(f" Missing: {csv_path}")
        continue

    df = pd.read_csv(csv_path)

    ax = axes[idx]
    scatter = ax.scatter(
        df["TSNE-1"], df["TSNE-2"],
        c=df["ClusterID"],
        cmap="tab10",
        edgecolors='k',
        s=50,
        alpha=0.9
    )
    ax.set_title(f"{model} ‚Äî t-SNE Clusters", fontsize=14)
    ax.set_xlabel("t-SNE-1")
    ax.set_ylabel("t-SNE-2")

# Remove any extra subplot space
for j in range(len(models), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.savefig("results/all_models_tsne_clusters.png", dpi=400)
plt.show()

import os
import numpy as np
import pandas as pd
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import rdMolDraw2D
import matplotlib.pyplot as plt
from rdkit import Chem
from rdkit.Chem import Draw
from PIL import Image
import io
import cairosvg

# Load smiles and dipole data
csv_path = "qm9/raw/gdb9.sdf.csv"
sdf_path = "qm9/raw/gdb9.sdf"

qm9_df = pd.read_csv(csv_path)
with open(sdf_path, "r") as f:
    sdf_data = f.read().split("$$$$\n")

smiles_list = []
for mol_block in sdf_data[:10000]:
    mol = Chem.MolFromMolBlock(mol_block, sanitize=False, removeHs=False)
    if mol:
        smiles_list.append(Chem.MolToSmiles(mol))
    else:
        smiles_list.append(None)

# Filter SMILES and dipoles for test set (9000:10000)
test_smiles = smiles_list[9000:10000]
test_mu = qm9_df["mu"].values[9000:10000]

# Models
model_names = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]

for model in model_names:
    print(f"Processing {model}")
    # === Load latent vectors ===
    latent_path = f"results/{model}_latent.npy"
    latent = np.load(latent_path)

    # === t-SNE projection ===
    tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=42)
    tsne_coords = tsne.fit_transform(latent)

    # === KMeans Clustering ===
    kmeans = KMeans(n_clusters=5, random_state=42).fit(tsne_coords)
    cluster_ids = kmeans.labels_

    # === Build DataFrame ===
    df = pd.DataFrame({
        "SMILES": test_smiles,
        "DipoleMoment_Œº": test_mu,
        "ClusterID": cluster_ids,
        "TSNE-1": tsne_coords[:, 0],
        "TSNE-2": tsne_coords[:, 1]
    })

    # === Save CSV ===
    csv_out = f"results/{model}_tsne_clustered.csv"
    df.to_csv(csv_out, index=False)
    print(f"Saved clustered data: {csv_out}")

    # === Visualize 12 representative molecules ===
    cluster_mols = []
    for cluster_id in range(5):
        reps = df[df["ClusterID"] == cluster_id].head(3)  # 3 from each of 5 clusters = 15 total
        for smi in reps["SMILES"]:
            mol = Chem.MolFromSmiles(smi)
            if mol:
                cluster_mols.append(mol)


def save_svg_grid_as_png(mols, filename, legends=None):
    # Step 1: Draw grid as SVG string
    svg = Draw.MolsToGridImage(
        mols,
        molsPerRow=5,
        subImgSize=(200, 200),
        legends=legends or ["" for _ in mols],
        useSVG=True
    )

    # Step 2: Convert SVG string to PNG using cairosvg
    png_bytes = cairosvg.svg2png(bytestring=svg.encode('utf-8'))

    # Step 3: Load PNG into PIL and save
    image = Image.open(io.BytesIO(png_bytes))
    image.save(filename)
    print(f"PNG saved: {filename}")

import pandas as pd
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import rdMolDraw2D
import os

def save_molecule_grid(mols, filename, molsPerRow=3, subImgSize=(200, 200)):
    drawer = rdMolDraw2D.MolDraw2DCairo(
        molsPerRow * subImgSize[0],
        ((len(mols) + molsPerRow - 1) // molsPerRow) * subImgSize[1],
        subImgSize[0],
        subImgSize[1]
    )
    drawer.DrawMolecules(mols)
    drawer.FinishDrawing()
    with open(filename, "wb") as f:
        f.write(drawer.GetDrawingText())
    print(f"Molecule grid saved to {filename}")

# Process all models
models = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]

for model in models:
    print(f"Processing {model}...")
    df = pd.read_csv(f"results/{model}_tsne_clustered.csv")

    # Pick 3 top cluster SMILES
    mols = []
    for cluster_id in df["ClusterID"].value_counts().nlargest(3).index.tolist():
        smi = df[df["ClusterID"] == cluster_id].iloc[0]["SMILES"]
        mol = Chem.MolFromSmiles(smi)
        if mol:
            mols.append(mol)

    save_molecule_grid(mols, f"results/{model}_clusters_molecules.png")

import matplotlib.pyplot as plt
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import rdMolDraw2D
from PIL import Image, ImageDraw, ImageFont
import io
import pandas as pd
import matplotlib.patches as patches

models = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
num_clusters = 5
examples_per_cluster = 1

cluster_colors = ['red', 'green', 'blue', 'purple', 'orange']

def get_clustered_mols(model, cluster_id):
    df = pd.read_csv(f"results/{model}_tsne_clustered.csv")
    subset = df[df["ClusterID"] == cluster_id].head(examples_per_cluster)
    mols = [Chem.MolFromSmiles(smi) for smi in subset["SMILES"]]
    labels = [f"Œº = {mu:.2f} D" for mu in subset["DipoleMoment_Œº"]]
    return mols, labels

def draw_mol_with_label(mol, label, size=(200, 200)):
    drawer = rdMolDraw2D.MolDraw2DCairo(size[0], size[1] - 30, size[0], size[1] - 30)
    drawer.DrawMolecule(mol)
    drawer.FinishDrawing()
    mol_img = Image.open(io.BytesIO(drawer.GetDrawingText()))

    canvas = Image.new('RGBA', size, (255, 255, 255, 255))
    canvas.paste(mol_img, (0, 0))

    draw = ImageDraw.Draw(canvas)
    font = ImageFont.load_default()
    draw.text((10, size[1] - 25), label, fill=(0, 0, 0), font=font)

    return canvas.convert('RGB')

# Set up matplotlib figure
fig, axes = plt.subplots(nrows=num_clusters, ncols=len(models), figsize=(len(models)*2.5, num_clusters*2.5))

for col, model in enumerate(models):
    for row in range(num_clusters):
        mols, labels = get_clustered_mols(model, row)
        img = draw_mol_with_label(mols[0], labels[0])

        axes[row, col].imshow(img)
        axes[row, col].axis("off")

        if row == 0:
            axes[row, col].set_title(model, fontsize=12)

        # Draw border
        for spine in axes[row, col].spines.values():
            spine.set_edgecolor(cluster_colors[row % len(cluster_colors)])
            spine.set_linewidth(2)

# Adjust and save
plt.tight_layout()
plt.savefig("results/all_models_clusters_colored_labeled.png", dpi=300)
plt.show()

from rdkit import Chem
import pandas as pd
import os

# Path to the SDF file from QM9
sdf_path = "qm9/raw/gdb9.sdf"
output_csv = "results/smiles_test.csv"

os.makedirs("results", exist_ok=True)

# Read molecules from SDF
supplier = Chem.SDMolSupplier(sdf_path, removeHs=False)

# Extract valid SMILES from test set (9000‚Äì9999)
valid_smiles = []
start, end = 9000, 10000
for i in range(start, end):
    mol = supplier[i]
    if mol is not None:
        try:
            smi = Chem.MolToSmiles(mol)
            valid_smiles.append(smi)
        except:
            print(f" Failed to convert molecule at index {i}")

# Save to CSV
df = pd.DataFrame({"SMILES": valid_smiles})
df.to_csv(output_csv, index=False)
print(f"Extracted {len(valid_smiles)} SMILES to {output_csv}")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import imageio.v2 as imageio
import os

def animate_tsne(model_name, iterations=[250, 500, 750, 1000], perplexity=30, learning_rate=200):
    print(f"Generating t-SNE animation for {model_name}...")

    latent_path = f"results/{model_name}_latent.npy"
    dipole_path = f"results/{model_name}_y_true.npy"
    gif_path = f"results/{model_name}_tsne_evolution.gif"

    latent = np.load(latent_path)
    dipoles = np.load(dipole_path).squeeze()
    frames = []

    for n_iter in iterations:
        print(f" Running t-SNE with n_iter = {n_iter}")
        tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=learning_rate,
                    n_iter=n_iter, init='random', random_state=42)
        emb = tsne.fit_transform(latent)

        # Plot
        fig, ax = plt.subplots(figsize=(8, 6))
        fig.patch.set_facecolor('white')  # Ensures consistent white background

        scatter = ax.scatter(emb[:, 0], emb[:, 1], c=dipoles, cmap='plasma', s=12)
        ax.set_title(f"t-SNE Evolution (n_iter = {n_iter})", fontsize=14)
        ax.axis("off")
        fig.colorbar(scatter, ax=ax, label="Dipole Moment Œº")
        ax.text(0.05, 0.95, model_name, transform=ax.transAxes,
                fontsize=12, color='white', bbox=dict(facecolor='black', alpha=0.5))

        # Save frame
        temp_path = f"results/{model_name}_tsne_{n_iter}.png"
        fig.savefig(temp_path, dpi=100, bbox_inches='tight', facecolor=fig.get_facecolor())
        plt.close(fig)

        frames.append(imageio.imread(temp_path))

    # Save GIF
    imageio.mimsave(gif_path, frames, duration=1.2)  # Slower and cleaner
    print(f"t-SNE animation saved: {gif_path}")

    # Cleanup temporary frames
    for f in os.listdir("results"):
        if f.startswith(f"{model_name}_tsne_") and f.endswith(".png"):
            os.remove(os.path.join("results", f))

for model in ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]:
    animate_tsne(model)

from moviepy.editor import VideoFileClip, clips_array, vfx
import os

# === Configuration ===
models = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
gif_dir = "results"
output_path = os.path.join(gif_dir, "all_models_tsne_grid_slow.gif")
target_size = (320, 240)  # Width x Height for all clips

# === Load and process clips ===
clips = []
for model in models:
    gif_path = os.path.join(gif_dir, f"{model}_tsne_evolution.gif")
    if os.path.exists(gif_path):
        print(f" Loading: {gif_path}")
        clip = VideoFileClip(gif_path).resize(target_size).fx(vfx.speedx, factor=0.02)
        clips.append(clip)
    else:
        print(f" Missing GIF skipped: {gif_path}")

# === Ensure 2D layout (e.g., 4 columns per row) ===
n_cols = 4
rows = [clips[i:i+n_cols] for i in range(0, len(clips), n_cols)]
# Pad last row if needed
if len(rows[-1]) < n_cols:
    empty_clip = clips[0].copy().fx(vfx.freeze, t=0, freeze_duration=clips[0].duration)
    for _ in range(n_cols - len(rows[-1])):
        rows[-1].append(empty_clip)

# === Create and save grid animation ===
final_clip = clips_array(rows)
final_clip.write_gif(output_path, fps=5)
print(f"Grid animation saved to: {output_path}")

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import scipy.stats as stats

def plot_model_errors_landscape(model_names, results_dir="results", target_name="Dipole Moment Œº"):
    num_models = len(model_names)
    fig, axes = plt.subplots(2, num_models, figsize=(3.5 * num_models, 10))  # 2 rows: top = error, bottom = QQ

    for idx, model in enumerate(model_names):
        print(f"Processing errors for: {model}")
        y_true = np.load(f"{results_dir}/{model}_y_true.npy").squeeze()
        y_pred = np.load(f"{results_dir}/{model}_y_pred.npy").squeeze()
        errors = y_pred - y_true

        # Top row: Histogram + KDE
        ax1 = axes[0, idx]
        sns.histplot(errors, bins=40, kde=True, color='skyblue', edgecolor='black', ax=ax1)
        ax1.axvline(0, color='red', linestyle='--', label='Zero Error')
        ax1.set_title(f"{model}", fontsize=11)
        ax1.set_xlabel("Residuals")
        ax1.set_ylabel("Freq")
        ax1.grid(True, linestyle='--', alpha=0.5)

        # Bottom row: QQ Plot
        ax2 = axes[1, idx]
        stats.probplot(errors, dist="norm", plot=ax2)
        ax2.set_title("")  # optional
        ax2.set_xlabel("Theoretical Quantiles")
        ax2.set_ylabel("Ordered Residuals")
        ax2.grid(True)

    plt.suptitle("Prediction Residuals and QQ Plots Across Models", fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(f"{results_dir}/error_qq_all_models_landscape.png", dpi=400, bbox_inches='tight')
    plt.show()

model_names = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
plot_model_errors_landscape(model_names)

import torch
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.linear_model import LinearRegression

# === Data from real evaluation ===
df = pd.DataFrame({
    "Model": ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"],
    "Trainable Parameters": [
        6545, 12897, 15610, 36278, 46080, 14625, 21489
    ],
    "R¬≤ Score": [
        0.5583, 0.5874, 0.6213, 0.5141, 0.5097, 0.6004, 0.5858
    ],
    "Family": [
        "Conv", "Conv", "Conv", "Attention", "Attention", "Conv", "Hybrid"
    ]
})

# === Log-transform parameters for regression ===
df["log_params"] = np.log10(df["Trainable Parameters"])

# === Fit regression line ===
X = df["log_params"].values.reshape(-1, 1)
y = df["R¬≤ Score"].values
reg = LinearRegression().fit(X, y)
y_pred = reg.predict(X)

# === Plot ===
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=df,
    x="Trainable Parameters",
    y="R¬≤ Score",
    hue="Family",
    palette="Set2",
    s=100,
    edgecolor='black'
)

# Plot trendline
plt.plot(df["Trainable Parameters"], y_pred, color='gray', linestyle='--', label="Log-linear Trend")

# Annotate each point
for _, row in df.iterrows():
    plt.text(row["Trainable Parameters"], row["R¬≤ Score"] + 0.006, row["Model"],
             fontsize=9, ha='center')

plt.xscale("log")
plt.xlabel("Trainable Parameters (log scale)")
plt.ylabel("R¬≤ Score")
plt.title("Model Complexity vs. Accuracy")
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(title="Architecture Family")
plt.tight_layout()
plt.savefig("results/model_complex_vs_accuracy_labeled.png", dpi=400)
plt.show()

import os
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error

models = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
records = []

for model in models:
    try:
        train_loss = np.load(f"results/{model}_train_loss.npy")  # MSE
        val_loss = np.load(f"results/{model}_val_loss.npy")      # MSE
        y_true_val = np.load(f"results/{model}_y_true.npy")
        y_pred_val = np.load(f"results/{model}_y_pred.npy")

        # Estimate val MAE (already computed for test, reused for val)
        val_mae = mean_absolute_error(y_true_val[:1000], y_pred_val[:1000])  # assuming 1K val
        # Train MAE is unavailable unless explicitly saved, so we use np.nan
        records.append({
            "Model": model,
            "Train_MSE": train_loss[-1],
            "Val_MSE": val_loss[-1],
            "Train_MAE": np.nan,
            "Val_MAE": val_mae
        })
    except FileNotFoundError:
        print(f" Missing data for {model}, skipping.")

df = pd.DataFrame(records)
df.to_csv("results/train_val_metrics.csv", index=False)
print("Extended train_val_metrics.csv saved.")

import pandas as pd

# === Load benchmark metrics ===
results_path = "results/model_results.csv"
cluster_path = "results/clustering_scores.csv"
train_val_path = "results/train_val_metrics.csv"

model_df = pd.read_csv(results_path)
cluster_df = pd.read_csv(cluster_path)
train_val_df = pd.read_csv(train_val_path)

# === Merge on Model name ===
summary_df = pd.merge(model_df, cluster_df, on="Model")
summary_df = pd.merge(summary_df, train_val_df, on="Model")

# === Round for clarity ===
summary_df = summary_df.round({
    "Train_MSE": 4,
    "Val_MSE": 4,
    "Train_MAE": 4,
    "Val_MAE": 4,
    "Test_MSE": 4,
    "Test_MAE": 4,
    "Test_R2": 4,
    "Silhouette Score": 4,
    "Davies-Bouldin Index": 4,
    "Calinski-Harabasz Score": 2
})

# === Sort by best Test R¬≤ ===
summary_df = summary_df.sort_values(by="Test_R2", ascending=False)

# === Display and Save ===
print(summary_df.to_markdown(index=False))
summary_df.to_csv("results/summary_model_metrics.csv", index=False)

import os
import pandas as pd

# Define your model names and initialize list
model_names = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
summary_data = []

# Iterate over each model
for model in model_names:
    file_path = f"results/{model}_tsne_clustered.csv"

    if not os.path.exists(file_path):
        print(f" File not found: {file_path}")
        continue

    df = pd.read_csv(file_path)

    # Compute number of unique clusters and average dipole moment per cluster
    cluster_stats = df.groupby("ClusterID")["DipoleMoment_Œº"].agg(["mean", "count"]).reset_index()
    cluster_stats["Model"] = model
    summary_data.append(cluster_stats)

# Combine all into a single DataFrame
full_summary = pd.concat(summary_data, ignore_index=True)

# Rearrange and save
full_summary = full_summary[["Model", "ClusterID", "count", "mean"]]
full_summary.columns = ["Model", "Cluster ID", "Molecules per Cluster", "Avg Dipole Moment (Œº)"]
full_summary.to_csv("results/tsne_cluster_dipole_summary.csv", index=False)

print("Saved summary table to: results/tsne_cluster_dipole_summary.csv")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import umap
import os

# === Configuration ===
model_names = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
save_dir = "results"
os.makedirs(save_dir, exist_ok=True)

for model in model_names:
    print(f"Processing {model}")

    # Load latent and dipole values
    latent = np.load(f"{save_dir}/{model}_latent.npy")
    dipoles = np.load(f"{save_dir}/{model}_y_true.npy").squeeze()

    # --- 3D PCA ---
    pca = PCA(n_components=3)
    pca_emb = pca.fit_transform(latent)
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')
    p = ax.scatter(pca_emb[:, 0], pca_emb[:, 1], pca_emb[:, 2], c=dipoles, cmap='plasma', s=10)
    ax.set_title(f"{model} - 3D PCA")
    fig.colorbar(p, ax=ax, label='Dipole Moment Œº')
    plt.tight_layout()
    plt.savefig(f"{save_dir}/{model}_pca_3d.png", dpi=300)
    plt.close()

    # --- 3D UMAP ---
    reducer = umap.UMAP(n_components=3, random_state=42)
    umap_emb = reducer.fit_transform(latent)
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')
    p = ax.scatter(umap_emb[:, 0], umap_emb[:, 1], umap_emb[:, 2], c=dipoles, cmap='plasma', s=10)
    ax.set_title(f"{model} - 3D UMAP")
    fig.colorbar(p, ax=ax, label='Dipole Moment Œº')
    plt.tight_layout()
    plt.savefig(f"{save_dir}/{model}_umap_3d.png", dpi=300)
    plt.close()

    print(f"Saved PCA/UMAP plots for {model}")

import matplotlib.pyplot as plt
from PIL import Image
import os

# === Configuration ===
models = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
save_dir = "results"

def make_grid_panel(title, suffix, output_file):
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    axes = axes.flatten()

    for i, model in enumerate(models):
        img_path = os.path.join(save_dir, f"{model}_{suffix}.png")
        if not os.path.exists(img_path):
            print(f" Missing: {img_path}")
            continue
        img = Image.open(img_path)
        axes[i].imshow(img)
        axes[i].axis('off')
        axes[i].set_title(model, fontsize=14)

    # Remove extra subplot if models < axes
    for j in range(len(models), len(axes)):
        fig.delaxes(axes[j])

    plt.suptitle(title, fontsize=20)
    plt.tight_layout()
    plt.subplots_adjust(top=0.90)
    plt.savefig(os.path.join(save_dir, output_file), dpi=300)
    plt.show()

# === Generate Panels ===
make_grid_panel("3D PCA Projection of Latent Spaces", "pca_3d", "panel_latent_pca.png")
make_grid_panel("3D UMAP Projection of Latent Spaces", "umap_3d", "panel_latent_umap.png")

import torch
from torch_geometric.datasets import QM9
from torch_geometric.nn import GATConv
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import matplotlib as mpl
import os

# Load dataset and molecule
dataset = QM9(root="qm9")
data = dataset[0]

# Map atomic numbers to element symbols
atomic_symbols = {1: 'H', 6: 'C', 7: 'N', 8: 'O', 9: 'F'}

# Define a generic GAT model for attention visualization
class GATAttention(torch.nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.gat = GATConv(in_channels, 64, heads=1, concat=False)

    def forward(self, x, edge_index):
        out, (edges, attn_weights) = self.gat(x, edge_index, return_attention_weights=True)
        return out, edges, attn_weights

# Models to visualize
model_names = ["GCN", "GIN", "GraphConv", "GATNet", "GATConv", "SAGEConv", "GIN+EdgeConv"]
save_dir = "results/attention_maps"
os.makedirs(save_dir, exist_ok=True)

for model_name in model_names:
    if "GAT" not in model_name:
        print(f" Skipping {model_name} (no attention mechanism)")
        continue

    print(f"üîç Visualizing attention for {model_name}")
    model = GATAttention(dataset.num_node_features)
    model.eval()

    with torch.no_grad():
        _, edge_index, attn = model(data.x, data.edge_index)

    # Build graph
    G = nx.Graph()
    edges = edge_index.cpu().numpy()
    attn = attn.cpu().numpy().squeeze().astype(float)

    for i, (src, tgt) in enumerate(edges.T):
        G.add_edge(src, tgt, weight=attn[i])

    pos = nx.spring_layout(G, seed=42)
    weights = [G[u][v]['weight'] for u, v in G.edges()]

    # Normalize weights for colorbar
    norm = mpl.colors.Normalize(vmin=min(weights), vmax=max(weights))
    cmap = plt.cm.plasma

    # Use atomic labels
    labels = {
        i: atomic_symbols.get(int(z.item()), str(int(z.item())))
        for i, z in enumerate(data.z)
    }

    fig, ax = plt.subplots(figsize=(6, 6))
    nx.draw(
        G, pos,
        ax=ax,
        labels=labels,
        with_labels=True,
        node_color="lightblue",
        node_size=500,
        edge_color=weights,
        edge_cmap=cmap,
        width=2
    )

    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
    sm.set_array([])
    fig.colorbar(sm, ax=ax, label="Attention Weight")

    plt.title(f"{model_name} Attention Weights on Sample Molecule")
    plt.tight_layout()
    save_path = os.path.join(save_dir, f"{model_name}_attention_weights.png")
    plt.savefig(save_path, dpi=300)
    print(f"Saved: {save_path}")
    plt.show()

import shutil
import os

# Path to your results folder
folder_path = "results"
zip_filename = "results_archive.zip"

# Create a zip file
shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', folder_path)

# Download the zip file
from google.colab import files
files.download(zip_filename)